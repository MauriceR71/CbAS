{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from seqtools import SequenceTools as ST\n",
    "from gfp_gp import SequenceGP\n",
    "from util import AA, AA_IDX\n",
    "from util import build_vae\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gan import WGAN\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from util import one_hot_encode_aa, partition_data, get_balaji_predictions, get_samples\n",
    "from util import convert_idx_array_to_aas, build_pred_vae_model, get_experimental_X_y\n",
    "from util import get_gfp_X_y_aa\n",
    "from losses import neg_log_likelihood\n",
    "import json\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(M):\n",
    "    x = Input(shape=(M, 20,))\n",
    "    y = Flatten()(x)\n",
    "    y = Dense(50, activation='elu')(y)\n",
    "    y = Dense(2)(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "def evaluate_ground_truth(X_aa, ground_truth, save_file=None):\n",
    "    y_gt = ground_truth.predict(X_aa, print_every=100000)[:, 0]\n",
    "    if save_file is not None:\n",
    "        np.save(save_file, y_gt)\n",
    "        \n",
    "def train_and_save_oracles(X_train, y_train, n=10, suffix='', batch_size=100):\n",
    "    for i in range(n):\n",
    "        model = build_model(X_train.shape[1])\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=neg_log_likelihood,\n",
    "                      )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', \n",
    "                                   min_delta=0, \n",
    "                                   patience=5, \n",
    "                                   verbose=1)\n",
    "\n",
    "        model.fit(X_train, y_train, \n",
    "                  epochs=100, \n",
    "                  batch_size=batch_size, \n",
    "                  validation_split=0.1, \n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=2)\n",
    "        model.save(\"models/oracle_%i%s.h5\" % (i, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ml_opt(X_train, oracles, ground_truth, vae_0, weights_type='dbas',\n",
    "        LD=20, iters=20, samples=500, homoscedastic=False, homo_y_var=0.1,\n",
    "        quantile=0.95, verbose=False, alpha=1, train_gt_evals=None,\n",
    "        cutoff=1e-6, it_epochs=10, enc1_units=50):\n",
    "    \n",
    "    assert weights_type in ['cbas', 'dbas','rwr', 'cem-pi', 'fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    vae = build_vae(latent_dim=LD,\n",
    "                    n_tokens=20, seq_length=L,\n",
    "                    enc1_units=enc1_units)\n",
    "\n",
    "    traj = np.zeros((iters, 7))\n",
    "    oracle_samples = np.zeros((iters, samples))\n",
    "    gt_samples = np.zeros((iters, samples))\n",
    "    oracle_max_seq = None\n",
    "    oracle_max = -np.inf\n",
    "    gt_of_oracle_max = -np.inf\n",
    "    y_star = - np.inf\n",
    "    for t in range(iters):\n",
    "        ### Take Samples ###\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        if t > 0:\n",
    "            Xt_p = vae.decoder_.predict(zt)\n",
    "            Xt = get_samples(Xt_p)\n",
    "        else:\n",
    "            Xt = X_train\n",
    "        \n",
    "        ### Evaluate ground truth and oracle ###\n",
    "        yt, yt_var = get_balaji_predictions(oracles, Xt)\n",
    "        if homoscedastic:\n",
    "            yt_var = np.ones_like(yt) * homo_y_var\n",
    "        Xt_aa = np.argmax(Xt, axis=-1)\n",
    "        if t == 0 and train_gt_evals is not None:\n",
    "            yt_gt = train_gt_evals\n",
    "        else:\n",
    "            yt_gt = ground_truth.predict(Xt_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        ### Calculate weights for different schemes ###\n",
    "        if t > 0:\n",
    "            if weights_type == 'cbas': \n",
    "                log_pxt = np.sum(np.log(Xt_p) * Xt, axis=(1, 2))\n",
    "                X0_p = vae_0.decoder_.predict(zt)\n",
    "                log_px0 = np.sum(np.log(X0_p)*Xt, axis=(1, 2))\n",
    "                w1 = np.exp(log_px0-log_pxt)\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                w2= scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "                weights = w1*w2 \n",
    "            elif weights_type == 'cem-pi':\n",
    "                pi = scipy.stats.norm.sf(max_train_gt, loc=yt, scale=np.sqrt(yt_var))\n",
    "                pi_thresh = np.percentile(pi, quantile*100)\n",
    "                weights = (pi > pi_thresh).astype(int)\n",
    "            elif weights_type == 'dbas':\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                weights = scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "            elif weights_type == 'rwr':\n",
    "                weights = np.exp(alpha*yt)\n",
    "                weights /= np.sum(weights)\n",
    "#             elif weights_type == ''\n",
    "        else:\n",
    "            weights = np.ones(yt.shape[0])\n",
    "            max_train_gt = np.max(yt_gt)\n",
    "            \n",
    "        yt_max_idx = np.argmax(yt)\n",
    "        yt_max = yt[yt_max_idx]\n",
    "        if yt_max > oracle_max:\n",
    "            oracle_max = yt_max\n",
    "            try:\n",
    "                oracle_max_seq = convert_idx_array_to_aas(Xt_aa[yt_max_idx-1:yt_max_idx])[0]\n",
    "            except IndexError:\n",
    "                print(Xt_aa[yt_max_idx-1:yt_max_idx])\n",
    "            gt_of_oracle_max = yt_gt[yt_max_idx]\n",
    "        \n",
    "        ### Record and print results ##\n",
    "        if t == 0:\n",
    "            rand_idx = np.random.randint(0, len(yt), samples)\n",
    "            oracle_samples[t, :] = yt[rand_idx]\n",
    "            gt_samples[t, :] = yt_gt[rand_idx]\n",
    "        if t > 0:\n",
    "            oracle_samples[t, :] = yt\n",
    "            gt_samples[t, :] = yt_gt\n",
    "        \n",
    "        traj[t, 0] = np.max(yt_gt)\n",
    "        traj[t, 1] = np.mean(yt_gt)\n",
    "        traj[t, 2] = np.std(yt_gt)\n",
    "        traj[t, 3] = np.max(yt)\n",
    "        traj[t, 4] = np.mean(yt)\n",
    "        traj[t, 5] = np.std(yt)\n",
    "        traj[t, 6] = np.mean(yt_var)\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, traj[t, 0], color.BOLD + str(traj[t, 1]) + color.END, \n",
    "                  traj[t, 2], traj[t, 3], color.BOLD + str(traj[t, 4]) + color.END, traj[t, 5], traj[t, 6])\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae.encoder_.set_weights(vae_0.encoder_.get_weights())\n",
    "            vae.decoder_.set_weights(vae_0.decoder_.get_weights())\n",
    "            vae.vae_.set_weights(vae_0.vae_.get_weights())\n",
    "        else:\n",
    "            cutoff_idx = np.where(weights < cutoff)\n",
    "            Xt = np.delete(Xt, cutoff_idx, axis=0)\n",
    "            yt = np.delete(yt, cutoff_idx, axis=0)\n",
    "            weights = np.delete(weights, cutoff_idx, axis=0)\n",
    "            vae.fit([Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                  epochs=it_epochs,\n",
    "                  batch_size=10,\n",
    "                  shuffle=False,\n",
    "                  sample_weight=[weights, weights],\n",
    "                  verbose=0)\n",
    "            \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    return traj, oracle_samples, gt_samples, max_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_opt(X_train, oracles, ground_truth, vae_0, weights_type='fbvae',\n",
    "        LD=20, iters=20, samples=500, \n",
    "        quantile=0.8, verbose=False, train_gt_evals=None,\n",
    "        it_epochs=10, enc1_units=50):\n",
    "    \n",
    "    assert weights_type in ['fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    vae = build_vae(latent_dim=LD,\n",
    "                    n_tokens=20, seq_length=L,\n",
    "                    enc1_units=enc1_units)\n",
    "\n",
    "    traj = np.zeros((iters, 7))\n",
    "    oracle_samples = np.zeros((iters, samples))\n",
    "    gt_samples = np.zeros((iters, samples))\n",
    "    oracle_max_seq = None\n",
    "    oracle_max = -np.inf\n",
    "    gt_of_oracle_max = -np.inf\n",
    "    y_star = - np.inf\n",
    "    for t in range(iters):\n",
    "        ### Take Samples and evaluate ground truth and oracle ##\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        if t > 0:\n",
    "            Xt_sample_p = vae.decoder_.predict(zt)\n",
    "            Xt_sample = get_samples(Xt_sample_p)\n",
    "            yt_sample, _ = get_balaji_predictions(oracles, Xt_sample)\n",
    "            Xt_aa_sample = np.argmax(Xt_sample, axis=-1)\n",
    "            yt_gt_sample = ground_truth.predict(Xt_aa_sample, print_every=1000000)[:, 0]\n",
    "        else:\n",
    "            Xt = X_train\n",
    "            yt, _ = get_balaji_predictions(oracles, Xt)\n",
    "            Xt_aa = np.argmax(Xt, axis=-1)\n",
    "            fb_thresh = np.percentile(yt, quantile*100)\n",
    "            if train_gt_evals is not None:\n",
    "                yt_gt = train_gt_evals\n",
    "            else:\n",
    "                yt_gt = ground_truth.predict(Xt_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        ### Calculate threshold ###\n",
    "        if t > 0:\n",
    "            threshold_idx = np.where(yt_sample >= fb_thresh)[0]\n",
    "            n_top = len(threshold_idx)\n",
    "            sample_arrs = [Xt_sample, yt_sample, yt_gt_sample, Xt_aa_sample]\n",
    "            full_arrs = [Xt, yt, yt_gt, Xt_aa]\n",
    "            \n",
    "            for l in range(len(full_arrs)):\n",
    "                sample_arr = sample_arrs[l]\n",
    "                full_arr = full_arrs[l]\n",
    "                sample_top = sample_arr[threshold_idx]\n",
    "                full_arr = np.concatenate([sample_top, full_arr])\n",
    "                full_arr = np.delete(full_arr, range(full_arr.shape[0]-n_top, full_arr.shape[0]), axis=0)\n",
    "                full_arrs[l] = full_arr\n",
    "            Xt, yt, yt_gt, Xt_aa = full_arrs\n",
    "        yt_max_idx = np.argmax(yt)\n",
    "        yt_max = yt[yt_max_idx]\n",
    "        if yt_max > oracle_max:\n",
    "            oracle_max = yt_max\n",
    "            try:\n",
    "                oracle_max_seq = convert_idx_array_to_aas(Xt_aa[yt_max_idx-1:yt_max_idx])[0]\n",
    "            except IndexError:\n",
    "                print(Xt_aa[yt_max_idx-1:yt_max_idx])\n",
    "            gt_of_oracle_max = yt_gt[yt_max_idx]\n",
    "        \n",
    "        ### Record and print results ##\n",
    "\n",
    "        rand_idx = np.random.randint(0, len(yt), samples)\n",
    "        oracle_samples[t, :] = yt[rand_idx]\n",
    "        gt_samples[t, :] = yt_gt[rand_idx]\n",
    "\n",
    "        traj[t, 0] = np.max(yt_gt)\n",
    "        traj[t, 1] = np.mean(yt_gt)\n",
    "        traj[t, 2] = np.std(yt_gt)\n",
    "        traj[t, 3] = np.max(yt)\n",
    "        traj[t, 4] = np.mean(yt)\n",
    "        traj[t, 5] = np.std(yt)\n",
    "        if t > 0:\n",
    "            traj[t, 6] = n_top\n",
    "        else:\n",
    "            traj[t, 6] = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, traj[t, 0], color.BOLD + str(traj[t, 1]) + color.END, \n",
    "                  traj[t, 2], traj[t, 3], color.BOLD + str(traj[t, 4]) + color.END, traj[t, 5], traj[t, 6])\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae.encoder_.set_weights(vae_0.encoder_.get_weights())\n",
    "            vae.decoder_.set_weights(vae_0.decoder_.get_weights())\n",
    "            vae.vae_.set_weights(vae_0.vae_.get_weights())\n",
    "        else:\n",
    "        \n",
    "            vae.fit([Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                  epochs=1,\n",
    "                  batch_size=10,\n",
    "                  shuffle=False,\n",
    "                  verbose=0)\n",
    "            \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    return traj, oracle_samples, gt_samples, max_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_oracles():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    i = 1\n",
    "    num_models = [1, 5, 20]\n",
    "    for i in range(len(num_models)):\n",
    "        RANDOM_STATE = i+1\n",
    "        nm = num_models[i]\n",
    "        X_train, y_train, _  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "        suffix = '_%s_%i_%i' % (train_size_str, nm, RANDOM_STATE)\n",
    "        train_and_save_oracles(X_train, y_train, batch_size=10, n=nm, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_vaes():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    suffix = '_%s' % train_size_str\n",
    "    for i in [0, 2]:\n",
    "        RANDOM_STATE = i + 1\n",
    "        X_train, _, _  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "        vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "        vae_0.fit([X_train], [X_train, np.zeros(X_train.shape[0])],\n",
    "                  epochs=100,\n",
    "                  batch_size=10,\n",
    "                  verbose=2)\n",
    "        vae_0.encoder_.save_weights(\"models/vae_0_encoder_weights%s_%i.h5\"% (suffix, RANDOM_STATE))\n",
    "        vae_0.decoder_.save_weights(\"models/vae_0_decoder_weights%s_%i.h5\"% (suffix, RANDOM_STATE))\n",
    "        vae_0.vae_.save_weights(\"models/vae_0_vae_weights%s_%i.h5\"% (suffix, RANDOM_STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_experimental_vaes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experimental_weighted_ml(it, repeats=3):\n",
    "    \n",
    "    assert it in [0, 1, 2]\n",
    "    \n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    num_models = [1, 5, 20][it]\n",
    "    RANDOM_STATE = it + 1\n",
    "    \n",
    "    X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "    oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "    \n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    ground_truth = SequenceGP(load=True, load_prefix=\"data/gfp_gp\")\n",
    "    \n",
    "    loss = neg_log_likelihood\n",
    "    get_custom_objects().update({\"neg_log_likelihood\": loss})\n",
    "    oracles = [load_model(\"models/oracle_%i%s.h5\" % (i, oracle_suffix)) for i in range(num_models)]\n",
    "    \n",
    "    test_kwargs = [\n",
    "#                    {'weights_type':'cbas', 'quantile': 1},\n",
    "#                    {'weights_type':'rwr', 'alpha': 20},\n",
    "#                    {'weights_type':'dbas', 'quantile': 0.95},\n",
    "#                    {'weights_type':'cem-pi', 'quantile': 0.8},\n",
    "        {'weights_type': 'fbvae', 'quantile': 0.8}\n",
    "    ]\n",
    "    \n",
    "    base_kwargs = {\n",
    "        'homoscedastic': False,\n",
    "        'homo_y_var': 0.01,\n",
    "        'train_gt_evals':gt_train,\n",
    "        'samples':100,\n",
    "        'cutoff':1e-6,\n",
    "        'it_epochs':10,\n",
    "        'verbose':True,\n",
    "        'LD': 20,\n",
    "        'enc1_units':50,\n",
    "        'iters':1000\n",
    "    }\n",
    "    \n",
    "    if num_models==1:\n",
    "        base_kwargs['homoscedastic'] = True\n",
    "        base_kwargs['homo_y_var'] = np.mean((get_balaji_predictions(oracles, X_train)[0] - y_train)**2)\n",
    "    \n",
    "    for k in range(repeats):\n",
    "        for j in range(len(test_kwargs)):\n",
    "            test_name = test_kwargs[j]['weights_type']\n",
    "            suffix = \"_%s_%i_%i\" % (train_size_str, RANDOM_STATE, k)\n",
    "            if test_name == 'fbvae':\n",
    "#                 suffix = suffix + \"_%.2f\" % test_kwargs[j]['quantile']\n",
    "                if base_kwargs['iters'] == 1000:\n",
    "                    suffix += '_long'\n",
    "            \n",
    "                print(suffix)\n",
    "                kwargs = {}\n",
    "                kwargs.update(test_kwargs[j])\n",
    "                kwargs.update(base_kwargs)\n",
    "                [kwargs.pop(k) for k in ['homoscedastic', 'homo_y_var', 'cutoff', 'it_epochs']]\n",
    "                test_traj, test_oracle_samples, test_gt_samples, test_max = fb_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "            else:\n",
    "                kwargs = {}\n",
    "                kwargs.update(test_kwargs[j])\n",
    "                kwargs.update(base_kwargs)\n",
    "                test_traj, test_oracle_samples, test_gt_samples, test_max = weighted_ml_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "                \n",
    "            np.save('results/%s_traj%s.npy' %(test_name, suffix), test_traj)\n",
    "            np.save('results/%s_oracle_samples%s.npy' % (test_name, suffix), test_oracle_samples)\n",
    "            np.save('results/%s_gt_samples%s.npy'%(test_name, suffix), test_gt_samples )\n",
    "\n",
    "            with open('results/%s_max%s.json'% (test_name, suffix), 'w') as outfile:\n",
    "                json.dump(test_max, outfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cbas_q(qs = [0.5, 0.75, 0.95, 1]):\n",
    "    it = 0\n",
    "    \n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    num_models = [1, 5, 20][it]\n",
    "    RANDOM_STATE = it + 1\n",
    "    \n",
    "    X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "    oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "    \n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    ground_truth = SequenceGP(load=True, load_prefix=\"data/gfp_gp\")\n",
    "    \n",
    "    loss = neg_log_likelihood\n",
    "    get_custom_objects().update({\"neg_log_likelihood\": loss})\n",
    "    oracles = [load_model(\"models/oracle_%i%s.h5\" % (i, oracle_suffix)) for i in range(num_models)]\n",
    "    \n",
    "    test_kwargs = [ {'weights_type':'cbas', 'quantile': q} for q in qs]\n",
    "    \n",
    "    base_kwargs = {\n",
    "        'homoscedastic': False,\n",
    "        'homo_y_var': 0.01,\n",
    "        'train_gt_evals':gt_train,\n",
    "        'samples':100,\n",
    "        'cutoff':1e-6,\n",
    "        'it_epochs':10,\n",
    "        'verbose':True,\n",
    "        'LD': 20,\n",
    "        'enc1_units':50,\n",
    "        'iters':100\n",
    "    }\n",
    "    \n",
    "    if num_models==1:\n",
    "        base_kwargs['homoscedastic'] = True\n",
    "        base_kwargs['homo_y_var'] = np.mean((get_balaji_predictions(oracles, X_train)[0] - y_train)**2)\n",
    "    \n",
    "    for j in range(len(test_kwargs)):\n",
    "        test_name = test_kwargs[j]['weights_type']\n",
    "        qj = test_kwargs[j]['quantile']\n",
    "        suffix = \"_qtest_%s_%i_%.2f\" % (train_size_str, RANDOM_STATE, qj)\n",
    "        print(suffix)\n",
    "        kwargs = {}\n",
    "        kwargs.update(test_kwargs[j])\n",
    "        kwargs.update(base_kwargs)\n",
    "        test_traj, test_oracle_samples, test_gt_samples, test_max = weighted_ml_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "        \n",
    "        np.save('results/%s_traj%s.npy' %(test_name, suffix), test_traj)\n",
    "        np.save('results/%s_oracle_samples%s.npy' % (test_name, suffix), test_oracle_samples)\n",
    "        np.save('results/%s_gt_samples%s.npy'%(test_name, suffix), test_gt_samples )\n",
    "\n",
    "        with open('results/%s_max%s.json'% (test_name, suffix), 'w') as outfile:\n",
    "            json.dump(test_max, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_experimental_vaes()\n",
    "# train_experimental_oracles()\n",
    "\n",
    "# for i in range(1):\n",
    "#     run_experimental_weighted_ml(i, repeats=1)\n",
    "\n",
    "test_cbas_q(qs=[0.5, 0.75, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_density():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    RANDOM_STATE = it + 1\n",
    "\n",
    "    X_train, y_train, gt_train, X_test, y_test, gt_test  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE, return_test=True)\n",
    "\n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "\n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "              n_tokens=20, \n",
    "              seq_length=X_train.shape[1],\n",
    "              enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    X_shuffle = np.zeros_like(X_train)\n",
    "    for i in range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
